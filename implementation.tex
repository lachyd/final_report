\documentclass[capstone_report.tex]{subfiles}
\externaldocument{designdevelopment.tex}
\begin{document}
\chapter{System development}
This chapter outlines, in a relatively fixed chronological order, the activities undertaken in pursuance of each of the milestones previously enumerated. \\

\section{Construction of URSA}
The first step in our project was to build the acquired Erle-Copter and fit it out with sensors specific to URSA.  The most time consuming aspect of the physical construction of URSA was building customized mounts for our specific sensor hardware.\\

\subsection{Combined Hokuyo \& Ultrasonic Mount}
In this section we discuss how sensor mounts were designed in 3D modelling software and printed. 3D printing reduced time to build a prototype and allowed for multiple iterations to be carried out.\\

Figure \ref{fig:mount1} shows all models generated for the first design iteration.  All components except (4) were modeled by the URSA team using empirical measurements. The anti-vibration platform was coarsely modeled. Only the forward facing camera mesh was obtained from open source Erle copter models \cite{erle_camera_case} to ensure clearance from the laser mount.\\

To maximise usage of available space the platform was designed to be dual purpose, housing both the scanner and ultrasonic module.  The platform for the ultrasonic component was raised to ensure the beam was not obstructed.  The measurement beam can be modelled as a cone originating from the sensor module with an angle of \SI{15}{\degree}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/mount1_labelled.png}
    \caption{First iteration, dual laser scanner and ultrasonic mount. (1) Hokuyo URG01 (2) Ultrasonic module (3) Laser scanner mount designed by URSA (4) Mesh of Erle brain lid (5) Erle brain (6) Anti-vibration bed (7) Erle copter top mounting plate.\label{fig:mount1}}
\end{figure}

\subsection{Ultrasonic Mount - Version 1}

The first iteration worked as a laser mount but did not allow for an ultrasonic component to be mounted facing the ground.  Taking range measurements from the drone to the floor were considered more reliable.  The ground is more likely to be a uniform surface, especially in emergency situations where the roof may have collapsed or wires/debris are hanging from the ceiling.  Furthermore, measurements are noisier with increased distance to reflective surface.  For our initial prototype the drone is likely to be closer to the ground than the ceiling in most situations.  As a result the ultrasonic mount was moved to the back of the landing legs.  In this position placement of upwards and downwards facing modules is possible.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./imgs/mount2_labelled.png}
        \caption{Second mount design allowing for top and bottom ultrasonic mounts. (1) Ultrasonic module (2) Mount design by URSA.}
        \label{fig:mount2}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./imgs/mount3_labelled.png}
        \caption{Third mount deign obviating need for screws. (1) Through-hole pins (2) Removed side screw fittings. \label{fig:mount3}}
    \end{subfigure}
    \caption{Inflation layer\label{fig:inflation_layer}}
\end{figure}

\subsection{Ultrasonic Mount - Version 2}

The first version of ultrasonic mount fit the landing legs of the drone and ultrasonic component.  However, due to the component being used it was difficult to find screws small enough to secure the module.  Given on campus 3D printing facilities, creating an updated model to remove the need for screws was the most time efficient design decision.  The side mounting screw fitting were also removed, the tight fit of the mount to the legs meant no additional securing was necessary.

\section{Interfacing with sensors}
    At this stage of implementation, we were able to undertake some test flights under manual pilot control. As expected, the test flight using APM as the flight controller exhibited some drift over time without GPS data. \\

    The next stage of the project was therefore to build a reliable interface between PX4 and the sensors which would use the PX4 bus. Once this has been achieved, we can replace the default APM flight controller with our version of PX4. In addition to expanding the PX4 code base to deal with our sensors, there are some additional considerations and issues which will be addressed in this section: 
    \begin{itemize}
        \item Choice and implementation of Raspberry Pi operating system to support PX4 sensor polling at required latencies.
        \item Hardware based signal conditioning of the SONAR output signal.
    \end{itemize}

    Interfacing with the LiDAR scanner will not be discussed in this section, since this is achieved within the ROS bus, rather than the PX4 bus. This aspect of the project will be discussed in the following section.\\

    \subsection{Raspberry Pi real-time operating system}
    When writing `bare-metal' code or programming for a microcontroller, it is safe to make various assumptions about timing. For example, if we were using an STM32 ARM microcontroller, and we recieved an interrupt signal, the time taken to respond to this signal would be deterministic and probably also very fast. It is important in time-critical systems such as URSA to have upper bounds on these response times. Clearly, if there is too long of a delay between a stimulus and an actuator response, the flight system could become unstable with no prospect of recovery. \\

    When writing a program designed to be executed within an operating system, this is not the case. Software executed on an operating system typically only has access to its own address space. Interacting with resources outside of this address space (for example, writing or reading from an I2C port) is accomplished by making a `system call' to the operating system. The operating system needs to manage the external resource requirements of many different applications running concurrently, as well as its own overhead. For example, many operating systems will disable interrupts during critical system tasks. It is impossible for an application to be confident that interrupts are available at any given time. Therefore, there is no guarantee that accessing an external resource, or being triggered by an external interrupt, will happen within any specific timeframe. This is a big problem for URSA, which has critical timing requirements in ensuring flight stability.\\

    Many embedded applications share this issue, however the resource management provided by an operating system is generally desirable for complex systems. In recognition of this, a class of operating systems known as `Real-time' operating systems (RTOS) was developed. An RTOS is designed with reliable interrupts in mind. The basic idea is that no program may disable interrupts, and even the most sensitive parts of the operating system must be designed in a way which supports being interrupted. This means that the only remaining variable in the time taken for an interrupt to be serviced is the priority of a process, and the number of other processes of equal or higher priority.\\

    There is a project within the Linux ecosystem known as \texttt{CONFIG\_PREEMPT\_RT}. This project is a patch which can be applied to modify the source code of the Linux kernel to support fully real-time capabilities. After applying this patch to the Linux kernel source code, we can compile this kernel and replace the default Raspbian kernel with our real-time variant. This will allow the Raspberry Pi to respond to interrupts with an acceptable level of reliability to control a flight platform in real-time.\\

    To compile the Linux kernel, we used the cross-compile tools provided by the Raspberry Pi foundation. This allowed us to compile the kernel on a desktop machine, despite the target platform being a 32-bit ARM processor. This significantly sped up the process. After cloning the Raspbian fork of the Linux kernel\footnote{Found at \url{https://github.com/raspberrypi/linux}}, we applied the \texttt{CONFIG\_PREEMPT\_RT} patch and enabled the fully pre-emptable kernel. We then built the kernel using the traditional Linux build process and GCC, having selected to cross compile for ARM 32 HF architecture.\\

    We replaced the existing kernel on our Raspbian system under \texttt{/boot} with the outputs from the above process. The system would boot, however it was observed that the Raspberry Pi would occasionally crash and need to be rebooted. It was found that these crashes were due to transferring data over the USB port, in particular a class of interrupts known as `fast interrupt requests' (FIQ).\\

    A simple workaround is to disable the FIQ capabilities of the USB driver. This was undertaken and provided good stability. The drawback is that USB data transfers now happen at a lower priority - however since we have minimal data requirements on the USB hub this will not be a problem for URSA. FIQ capabilities for the DWC USB controller may be disabled by adding \texttt{dwc\_otg.fiq\_enable=0 dwc\_otg.fiq\_fsm\_enable=0} to the file \texttt{/boot/cmdline.txt}. This file is run by the kernel for configuration when the system boots.\\

    As can be seen in Figure \ref{fig:PX4Arch}, some aspects of the PX4 flight controller require sensor readings at up to \SI{8}{\kilo\hertz}. Therefore, ideally we can achieve response times under \SI{125}{\micro\second} in order to support these components\footnote{In reality, PX4 can be configured to operate at slower speeds - however this is still a reasonable target to ensure stable flight}. Figure \ref{fig:RT_bench} shows some \texttt{cyclictest} benchmark data for Raspbian with the \texttt{CONFIG\_PREEMPT\_RT} patch applied, compared to a standard Raspbian installation.

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{imgs/rpi2-cyclictest-plot.png}
        \caption{Latency of Raspberry Pi with real-time patch (Source: \url{https://emlid.com/real-time-linux-for-raspberry-pi-2-and-updated-apm-binaries/})\label{fig:RT_bench}}
    \end{figure}

    As can be seen, there is a significant improvement observed in latency by applying the RT patch. We are reliably getting below \SI{100}{\micro\second} in response times, whereas previously we could expect over \SI{300}{\micro\second} depending on other tasks being performed by the OS. Our new kernel is therefore more appropriate for running PX4.

    \subsection{Signal conditioning of SONAR}
 	The HC-SR04 operates by transmitting a \SI{40}{\kilo\hertz} audio wave modulated by a series of pulses. A detector demodulates this signal by correlation, and the time between transmission and detection is measured. This time is related to distance by the speed of sound in air.\\

    The electrical interface for the HC-SR04 is a pulse width output, where the output of a pulse on one of the pins is proportional to the time between transmission and detection (and therefore also proportional to the distance measured). This is shown at Figure \ref{fig:sr04}.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{imgs/sr04.png}
        \caption{HC-SR04 timing\label{fig:sr04}}
    \end{figure}

After preliminary tests, it was found that there were fluctuations in UAV altitude readings even with the UAV held at a fixed altitude. This was further investigated via an oscilloscope. It was discovered that signals on the HC-SR04 echo pin occasionally experienced voltage drops. Additional impedances and noise introduced by the wires connecting the Raspberry Pi trigger/echo pins to the HC-SR04 were hypothesised to be the cause of the issue. \\

The problem is shown at Figure \ref{fig:badSig}, where URSA was held at a fixed height and a number of measurements were overlayed on each other. As can be seen, the pulse width is not constant between subsequent measurements, as should be the case.\\

As seen in Figure \ref{fig:badSig}, the fluctuation in pulse length experienced is approximately 4.2ms.  We can find the corresponding variation in measured distances by first noting the speed of sound is approximately \SI{340}{\metre\per\second}.  Given this observation we can derive the distance using:

\[
    x=\frac{340t}{2} = \SI{0.714}{\metre}
\]

This figure is within an order of magnitude of the altitude at which we are flying and therefore is unacceptable.\\

It was proposed that a filter/buffer PCB could be used to remove the erroneous signals, by both restoring the integrity of any damaged signals during transmission, and filtering any rapid changes. The final schematic consists of a comparator and linear regulator to ensure that voltage supplied to the Raspberry Pi GPIO did not exceed its \SI{3.3}{\volt} limits. This is shown at Figure \ref{fig:sigSchem}.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{imgs/schematic.png}
        \caption{Schematic for hardware signal conditioner\label{fig:sigSchem}}
    \end{figure}

We had this PCB fabricated and assembled. This led to significantly improved results. The waveform after hardware signal conditioning is shown at Figure \ref{fig:goodSig}. As can be seen, the jitter is reduced to a negligible amount.

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./imgs/sonar_conditioning.jpg}
            \caption{PCB}
            \label{fig:ultrasound_pcb}
        \end{subfigure}%
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./imgs/signal_board.jpg}
            \caption{Final board with HC-SR04 mounted}
            \label{fig:ultrasound_physical}
        \end{subfigure}
        \caption{Signal conditioning board}
    \end{figure}

Unfortunately, we did find that starting the motors re-introduced some minor jitter. We confirmed that this wasn't due to power supply issues. It appears likely that it is due to mechanical reasons - i.e. the rotors of the drone shake the connectors to the SONAR unit. Nevertheless, results were within acceptable limits for flying and testing the prototype unit.

    \begin{figure}[H]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{imgs/pulseBroad.jpg}
            \caption{Before conditioning}
            \label{fig:badSig}
        \end{subfigure}%
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{./imgs/pulseOk.jpg}
            \caption{After conditioning\label{fig:goodSig}}
        \end{subfigure}
        \caption{Amalgamation of sensor readings from a single height}
    \end{figure}


    \subsection{Sensor interface to PX4}
    There are two categories of sensors which were required to be interfaced with PX4. The first was the sensors provided with the Erle-copter. These sensors were mounted on a PCB which sits on top of the Raspberry Pi (the `Erle-brain'). The second category of sensors were additional sensors provided for the URSA project. The only one of these which was required to have a PX4 driver written for it was the SONAR driver.\\

    We consulted the manufacturer to determine whether drivers were already available. This was queried on the Erle-copter forums\footnote{\url{http://forum.erlerobotics.com/t/px4-on-eb3-access-to-drivers/2711}}, where it was confirmed that Erle Robotics do not provide PX4 source code or associated drivers. We therefore committed to building our own version of PX4 with drivers to support the hardware we had available.\\

    In order to build PX4 drivers for new hardware, there are two aspects which need to be understood in detail:
    \begin{itemize}
        \item The framework used by PX4 to interact with hardware; and,
        \item The physical layout and specifications of each hardware component.
    \end{itemize}

    Understanding the first point allows us to easily integrate our drivers with existing PX4 code; understanding the second point is necessary in designing the drivers themselves.\\

    \subsubsection{PX4 Driver Framework}
    PX4 has a specialised framework designed to allow abstraction and unification of drivers. This is called the `DriverFramework'. Some of the advantages of using DriverFramework are as follows:
    \begin{itemize}
        \item It has base classes for both I2C and serial peripheral interface (SPI), including implementations of the basic IO methods required to interface over these protocols.
        \item It has been thoroughly tested on portable operating system interface (POSIX) systems including for Raspberry Pi.
        \item There are defined methods for setting up drivers, allocating resources, and cleaning up. These methods are called as required by the DriverFramework.
        \item There are abstracted read/write methods which can be called simply by retrieving a handle to a driver. Handles can be acquired by using strings in the same format as a file system driver (e.g. `/dev/baro0' for a barometer).
    \end{itemize}

    More information on the DriverFramework can be found at \cite{dfoverview}. The basic design pattern requires calling various initialization methods on the driver. Once the driver is initialized and started, the framework will regularly call the \texttt{\_measure} method at the specified rate. This method can publish to \texttt{uORB} topics. Additionally, other parts of the PX4 code can obtain a handle to the driver and execute \texttt{read} and \texttt{write} methods on the driver. \\

    For our PX4 drivers, we follow the design pattern of providing a PX4 wrapper application which starts and stops each of our drivers. These wrappers are called by PX4 on startup, as specified in the PX4 configuration file. The source for these wrappers are located under \texttt{Firmware/src/platforms/posix/drivers}. The drivers themselves are written and contained within the DriverFramework, under \texttt{Firmware/src/lib/DriverFramework/drivers}. Note that all PX4 code for URSA is available on GitHub at \url{https://github.com/ursa-drone/Firmware}.

    \subsubsection{Hardware review}
    Having briefly considered the software framework in which our drivers will operate, we now look at the hardware for which drivers will be required. The main piece of hardware for which drivers are required is the Erle-Brain. This is a PCB with a number of chips relevant for flight sensing and control. It is designed to attach to the top of a Raspberry Pi, as a `hat'. It is shown at Figure \ref{fig:erleHat}, with points of interest numbered. In this Figure, it can be seen mounted on top of a Raspberry Pi.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{imgs/erleHat.jpg}
        \caption{Erle-Brain for Raspberry Pi\label{fig:erleHat}}
    \end{figure}

    \begin{table}[H]
    \centering
        \begin{tabular}{|p{1.8cm}|c|c|c|}
            \hline
            Reference on Figure \ref{fig:erleHat} & Description & Part number & Connection to Pi\\
            \hline
            1 & Power input & N/A & Via power pins \\
            2 & Raspberry Pi header & N/A & Direct \\
            3 & PWM Output pins & N/A & Connected to PWM controller \\
            4 & GPIO pin for RC input & N/A & GPIO 7 \\
            5 & PWM Controller & PCA9685 & I2C bus \\
            6 & Barometer & MS5611 & SPI bus using CE0 for chip select\\
            7 & Inertial measurement unit & MPU9250 & SPI bus using CE1 for chip select\\
            8 & ADC for battery life & ADS1115 & I2C bus \\
            9 & EEPROM - see below & N/A & ID\_SC and ID\_SD pins\\
            10 & I2C bus outputs & N/A & I2C bus \\
            11 & UART output & N/A & UART \\
            \hline
        \end{tabular}
    \caption{Components on Erle-brain\label{tab:ebcomponents}}
    \end{table}

    Erle Robotics does not provide documentation for the devices on the Erle-brain. We were required to reverse engineer this board in order to determine the devices, functions, and connections to the Pi for each component. This consisted of physically inspecting the component packages and PCB traces in order to determine the part numbers and connections. Our findings are shown at Table \ref{tab:ebcomponents}.\\

    In relation to the EERPOM component, we note that this is a requirement of so-called standardised `hats' for Raspberry Pi. Each hat requires an EEPROM module which contains information about the connected component, connected to reserved pins on the Pi. This is meant to allow easier software implementation of drivers; however in our case we found that the EEPROM on the Erle-brain had no useful information. It was therefore ignored for this project.\\

    The PWM chip provides significantly faster PWM frequencies than can be delivered by the Pi. It also frees up resources on the Pi, which only has a single hardware PWM output port. We require at least four PWM outputs (one for each motor).

    \subsubsection{Development of drivers for I2C \& SPI components}
    As can be seen in Table \ref{tab:ebcomponents}, four of the components on the Erle-Brain interact via relatively simple I2C/SPI protocols. As mentioned previously, the PX4 DriverFramework contains parent classes for I2C and SPI components. These classes interface with the relevant components by accessing the file system components exposed by the Raspbian operating system. \\

    For example, the Raspberry Pi has a single SPI interface which supports 2 channels, on pins CE0 and CE1. The device connected to pin CE0 is exposed in the file system as \texttt{/dev/spidev0.0}. Likewise, the device on pin CE1 is exposed as \texttt{/dev/spidev0.1}. These devices can be accessed using the general purpose \texttt{ioctl} system calls. The following code snippet illustrates how these devices may be accessed using system calls:

\begin{lstlisting}[language=c++]
#include <fcntl.h>
#include <sys/ioctl.h>     
#include <linux/spi/spidev.h>
#include <unistd.h>
int *spi_cs_fd;
*spi_cs_fd = open("/dev/spidev0.0", O_RDWR);
struct spi_ioc_transfer spi;
int length;
// Initialise properties of spi struct
// This includes pointer to data to write, pointer to data to read
// Initialise length
ioctl(*spi_cs_fd, SPI_IOC_MESSAGE(length), &spi);
\end{lstlisting}

    A more comprehensive version of the above code is implemented in PX4's DriverFramework, under the class SPIDevObj. A similar class exists called I2CDevObj. The main difference for this class is that an I2C address is also required to initialise this type of object. This is because while SPI uses a wire to select devices on the bus, I2C uses a code number at the beginning of the transmission. Some debugging of these base classes was required, due to our implementation on a Raspberry Pi. \\

    Using these base classes, we were able to subclass new driver classes for each of the components PCA9685, ADS1115, MS5611, and MPU9250. The driver classes contain methods which are derived from each components' datasheet. All datasheets have been included as appendices to this report. We also wrote wrappers which were able to initialise each of these classes, as mentioned previously. The source for each driver is included under the directories mentioned above.\\

    In the case of the PCA9685, this component also has an output enable pin for enabling PWM output. To control this pin, we used the GPIO driver which will be discussed below. This illustrates the power of using the PX4 DriverFramework - if any section of the code requires access to a device, it can simply request a handle to that device by name and execute read/write operations as required.

    \subsubsection{Development of GPIO drivers}
    In addition to the digital components described above, there are also two analog components within the URSA hardware ecosystem:
    \begin{itemize}
        \item The RC receiver
        \item The SONAR rangefinder 
    \end{itemize}

    The operation of the SONAR rangefinder has already been discussed in-depth. The RC receiver operates on a similar principle - analog signals proportional to pulse width - however it also allows multiplexing of different channels via pulse positional modulation (PPM). In this scheme, each of the channels represents one piece of data from the remote controller. This could be the position of a switch, the position of a joystick in the x-axis, the position of a joystick in the y-axis, etc. Each of these channels can be assigned a function within PX4 once they have been decoded, such as roll, pitch, yaw, land etc.\\

    An illustration of the waveform generated by the receiver is shown at Figure \ref{fig:ppm}.

    \begin{figure}[H]
    \centering
        \input{diagrams/ppm.tex}
        \caption{PPM illustration\label{fig:ppm}}
    \end{figure}

    The frame duration for PPM is approximately \SI{20}{\milli\second}, with each channel having a duration of between \SI{0.7}{\milli\second} to \SI{2.3}{\milli\second}. A channel pulse duration of \SI{1.5}{\milli\second} indicates a neutral value, for example a joystick at the center. To decode this signal, we need fast, regular sampling of the GPIO pin. We will need this same mechanism for the SONAR pin, so it is worth writing a general purpose fast GPIO driver which can be used by both of these other drivers (as well as the PCA9685 as mentioned above).\\

    The standard method of accessing the Raspberry Pi GPIO pins is through the Raspbian operating system. Pins can be exposed as file system objects, and written to/read from in a similar way to the SPI/I2C drivers mentioned above. However, by consulting Figure \ref{fig:RT_bench}, we can see that even with our real-time operating system, we can probably only expect to be sampling the GPIO every \SI{100}{\micro\second}. For a pulse with a range of \SI{1600}{\micro\second}, this gives us a quantization error of around 6\%. \\

    This is probably not acceptable for our purposes. In addition, since we cannot sample the GPIO at reliable intervals, we would need to also sample the clock register every sample. Further, having the CPU sample the GPIO registers so often would be very slow and likely cause issues with the other software running on the Raspberry Pi.\\

    A superior method for high frequency GPIO sampling is to swap the GPIO register into our program memory, interleaved with samples from the system clock register. This can be accomplished with only minimal CPU overhead at a very fast rate by using the direct memory access (DMA) peripheral which comes with the BCM2835 used by the Raspberry Pi. The datasheet for BCM2835 peripherals is included in the Appendices.\\

    To use the DMA capabilities, we need to provide a regular clock to trigger DMA transfers. This can be accomplished using the PCM module, also in the BCM2835 peripherals. PCM is a telephony/audio data protocol. It is possible to configure the PCM module to generate a DMA interrupt request whenever its FIFO buffer falls below a configurable level. This allows the DMA to fill up the FIFO again, ensuring uninterrupted data transfer. \\  

    In our case, we do not require the use of the Raspberry Pi's PCM capabilities. We can therefore load the PCM module with `dummy' data to initiate a transfer, and have the DMA interrupt request fire after just one bit has been transferred. This allows reliable control of the timing between successive DMA samples, at sampling times which can easily reach \SI{1}{\micro\second}. This process is shown at Figure \ref{fig:dma1}.


    \begin{figure}[H]
    \centering
        \input{diagrams/DMA_1.tex}
        \caption{DMA cycle\label{fig:dma1}}
    \end{figure}

    Once the GPIO register and timestamps are put into the program memory, it is simply a case of the program regularly looping through the timestamped samples to find transition times. This process is very fast, since the CPU can cache the block of memory containing timestamped samples and search it very quickly. The program can therefore achieve regular and reliable sampling at \SI{1}{\micro\second} while only executing this search loop at around \SI{1000}{\hertz}.\\

    We have presented a method for rapid sampling of the Raspberry Pi GPIO pins. The one aspect which remains is configuring the DMA instructions to execute the method shown at Figure \ref{fig:dma1}. This is actually non-trivial, since the DMA unit does not have its own memory for storing instructions. Instead, instructions need to be stored somewhere in our general purpose RAM. Note that each instruction points to the memory address of the next instruction, allowing an infinite loop. \\

    We therefore reserve some of the program space memory in PX4 to be dedicated to DMA control blocks. After this, we can configure the DMA unit to search for its instructions at the addresses reserved. We also need to reserve some buffer memory for the output of the control cycle, and ensure that this output memory is properly addressed in the DMA instructions. Configuration of the DMA and PCM units is done using the Linux system call \texttt{mmap(2)}\footnote{\url{http://man7.org/linux/man-pages/man2/mmap.2.html}} and the \texttt{/dev/mem} device file \footnote{\url{http://man7.org/linux/man-pages/man4/mem.4.html}}. \\

    The \texttt{/dev/mem} device file is necessary to allow `physical' addressing of memory. Typically, multiple programs can access the same address (say 0x00) without causing a conflict. This is because the memory addressing of a program is `virtual' and refers to the memory within that program's defined memory space. Addresses in the \texttt{/dev/mem} device file are interpreted as physical addresses, rather than virtual addresses. Physical addresses refer to the actual method in which components are hard-wired together. Most modern CPU architectures have a memory management unit (MMU) which maps virtual addresses to underlying physical addresses. \\

    If we can specify data registers using physical addresses, then we can use the addresses described in the BCM2835 datasheet to access peripheral registers. In the Raspberry Pi 3, physical addressing first encapsulates the RAM, with peripheral addressing starting at an offset of 0x3f000000 (which is around 1GB, the size of the RAM). Using physical addressing to access memory is a big security risk, and therefore our PX4 software will require super user permissions. \\

    Between the \texttt{mmap(2)} system call and \texttt{/dev/mem}, we can map a physical address (such as the configuration register for the DMA) into our program memory space, and read/write to and from this register in the same way we would read/write to and from a variable within our program. This allows us to easily configure the DMA and PCM peripherals. A simplified memory access architecture is shown at Figure \ref{fig:dma2}.

    \begin{figure}[H]
    \centering
        \input{diagrams/DMA_memory.tex}
        \caption{GPIO sampling memory architecture\label{fig:dma2}}
    \end{figure}

    During development and based on research conducted, we decided not to sample the system clock every second sample, as the timing from the PCM was reliable enough to interpolate. Instead, the clock is sampled once every 14 samples of the GPIO. The selection of this number 14 is explained below. \\

    It is desirable for memory management that both a full loop of DMA control blocks and a full loop of buffer outputs use an integer numbers of memory pages. This requires considering the size of each part of memory we plan to use.

    \begin{itemize}
        \item A page of memory is 4096 bytes under ARMv7 architecture
        \item A DMA control block is 32 bytes
        \item The GPIO register of interest is 4 bytes (32 pins)
        \item The high resolution clock register is 8 bytes
    \end{itemize}

    Taking 14 samples of GPIO and 1 clock sample requires 64 bytes of buffer. We need to do this 64 times to fill out a page of memory. For each 64 byte block of memory used, we require $(14\times2+1) = 29$ DMA instructions. This is because each sample of the GPIO requires one write to the PCM module to reset the sample timer. Since 64 and 29 do not have any obvious common multiples, we resort to reserving 29 pages of memory for the DMA instructions. This gives $29\times 4096 / 32 = 3712$ DMA instructions, which equates to $3712/29/64=2$ pages of timestamped GPIO buffer. So for every 29 pages of DMA instructions, we can get 2 pages of timestamped GPIO data. \\

    It is easy to multiply both sides of this equation out to increase the buffer - for example, we can use $x \times 29$ pages of DMA instructions to get $x \times 2$ pages of GPIO samples. Based on some experimentation, we found that $x=8$ was an appropriate value.\\

    This design was implemented in software under the DriverFramework driver \texttt{ursa\_gpio}. Each of the SONAR and RC drivers were implemented as PX4 modules only, rather than within the DriverFramework. This is because, with the high-speed GPIO sampling issue solved, neither of these modules required any further contact with the hardware. Callback functions were written to allow PX4 methods to be notified of level changes on GPIO pins, and these were used by both the SONAR and RC drivers. \\

    After implementing the GPIO driver module, we found that there were open source alternatives which use a similar design to the one outlined above. In particular, we found a module known as \texttt{PIGPIO}\footnote{\url{http://abyz.co.uk/rpi/pigpio/}}. This module is more comprehensive than our design, since it allows selection of the PWM peripheral for triggering the DMA (as an optional alternative to PCM), and also supports multiple sampling rates for the GPIO. Since this code was freely available under an appropriate license, we ported the relevant sections into our own code base whilst retaining most of our design.

    \subsubsection{Conclusion on sensor interfacing to PX4}
    We successfully achieved robust, fast GPIO sampling for the SONAR and RC receiver units. We were able to confirm that data from our sensors were available to other components within the PX4 ecosystem (such as the controller). We were also able to confirm that PX4 was capable of writing to our hardware (for example, writing commands to the PWM chip for output).

\section{Handheld Mapping}

After interfacing the drone with sensors, the next step is to construct a map while the drone is in a stationary position.   The goal is to send laser scans via WIFI to the ground-station where the data is interpreted by SLAM software.  Achieving this requires the completion of a series of intermediate steps:

\begin{itemize}
    \item Receive laser scan data (from one machine)
    \item Communicate with the drone via ground- station
    \item Visualize laser scanner data
    \item Interpret scans to estimate position within a map
    \item Clean laser scan data
\end{itemize}

\subsection{Receiving Laser Scan Data}
As mentioned previously packages are the primary organisation unit within ROS.  Packages are written to address a specific problem such as communicating laser scanner data or computing mathematical transfers.  They can be thought of as a folder containing a number of scripts, executables and other artefacts. The only required components are an XML package description and a CMakeLists.txt file for the build system.  Creating a package can either be done manually or via the utility command:

\begin{lstlisting}[language=bash]
    catkin_create_package <package name> <dependency 1> ...
\end{lstlisting}

Whereas a package is the software organisation unit of ROS, a node is the primary computational unit.  A node can be thought of as an executable that uses ROS to communicate with other nodes.  The simplest "computation graph" may be one node continually printing "hello world" and another node listening to the output.  This functionality is included for illustration in a debug package written by URSA \footnote{\url{https://github.com/ursa-drone/ursa-server/tree/master/src/debug}}.\\

  Nodes can perform any desired computational task, for example: filtering laser scan data, calculating costs associated with navigating an environment or printing the height data for an ultrasonic component.  In our case we want to interface the Hokuyo URG-04LX-UG01 with our drone.  We found that the "urg\_node" package works with any laser scanner adhering to the URG series communication protocol (SCIP) 2.2 or earlier.  An alternative method of installing a package if access to the source code is not required is simply$...$

\begin{lstlisting}[language=bash]
    sudo apt-get install ros-kinetic-urg-node
\end{lstlisting}

To enable communication between nodes, named buses called topics are used.  Topics are identified simply by strings.  For example a topic receiving laser scan data may be called "/scan".  Topics place few restrictions on their use by nodes.  A node wanting to receive data from a particular topic can subscribe to it.  A node wanting to send data to a particular topic may publish to it.  Multiple nodes can subscribe to one topic.  Similarly multiple nodes can publish to one topic.  A node publishing or subscribing to a topic is not aware of the node on the other end.  In this way the production and consumption of information is independent.   One important restriction placed on topics is that only one message type can be communicated over it.\\

A message is defined by a multi-line text file.  One column specifies data type while the other sets the names.  Text files are language neutral and therefore support cross-language development, which is another guiding principle of ROS.  Primitive data types such as float, int, string and array are allowed.  In addition, a message itself can be defined as a data type allowing unlimited nesting.\\

In this section we are primarily concerned with sending messages containing laser scan data.  A laser scan message is a common type in robotics and as such there is an existing definition used by package developers\footnote{\url{http://docs.ros.org/api/sensor_msgs/html/msg/LaserScan.html}}.  This message type is used by the urg\_node (Figure \ref{fig:message}).  Note that the majority of the fields are primitive types (float32 or float32[]).  "Header" is an example of a nested message type, which is commonly used to uniquely identify and timestamp data.

\begin{figure}[H]
    \begin{lstlisting}[language=bash]
std_msgs/Header header      # timestamps data from first ray
float32 angle_min           # start angle of scan
float32 angle_max           # end angle of scan
float32 angle_increment     # distance between measurements
float32 time_increment      # time between measurements 
float32 scan_time           # time between scans
float32 range_min           # minimum range values
float32 range_max           # maximum range value
float32[] ranges            # array containing ranges
float32[] intensities       # supported by select scanners  
    \end{lstlisting}
    \caption{Message definition file of LaserScan.msg}
    \label{fig:message}
\end{figure}

With the urg\_node installed, the follow commands begin data generation from the laser scanner:

\begin{lstlisting}[language=bash]
    Roscore
    Rosrun urg_node urg_node
\end{lstlisting}

Roscore is a command to start a collection of nodes and programs that are required to run a ROS based system. The Rosrun command instructs ROS to launch the urg\_node. The node ‘urg\_node’ is now running.  A number of utility commands exist within ROS to query or visualise the data being generated and ensure system operation is as expected.\\

To see the output of our laser scan data we type "rostopic echo /scan".  Figure \ref{fig:laser_snippet} shows a snippet of the raw data.  We can see that the data adheres to the typed definitions named in the .msg file.

\begin{figure}[H]
    \centering
\begin{lstlisting}[language=c++]
        header: 
          seq: 4195
          stamp: 
            secs: 1506932516
            nsecs: 774564046
          frame_id: laser
        angle_min: -2.35619449615
        angle_max: 2.09234976768
        angle_increment: 0.00613592332229
        time_increment: 9.76562732831e-05
        scan_time: 0.10000000149
        range_min: 0.019999999553
        range_max: 5.59999990463
        ranges: [0.01899999938905239, 0.01899999938905239, ...
    \end{lstlisting}
    \caption{Snippet of laser scan data}
    \label{fig:laser_snippet}
\end{figure}

\subsection{Communicate with the drone via ground-station}
Receiving data generated on one system (drone) on another system (ground-station) is simplified under the ROS framework. The basic requirements are to have bi-directional connectivity on all ports, one master and all using same ROS master URI. ROS\_MASTER\_URI is a Linux environment variable setup after the installation of ROS.  It can be altered directly in the terminal by setting the variable to the desired value.  In our case it makes sense to use the ground-station as the master, therefore we set this variable on the Raspberry Pi to be equal to the IP address of the ground-station.

\begin{lstlisting}[language=bash]
    export ROS_MASTER_URI = http://<hostname_of_groundstation>:11311
\end{lstlisting}

Under this setup we can now receive laser data on the ground-station. This was confirmed by executing rostopic echo on the ground-station.\\

Launching all of the nodes required by a complex system can become cumbersome. By way of example the commands needed to launch the application so far are given below (assuming code has been compiled).

\begin{lstlisting}[language=bash]
    # ground-station
    Rocore 
    Rostopic echo /scan

    # Raspberry Pi from groundstation
    ssh pi@<hostname_of_pi>
    cd catkin_ws
    source devel/setup.bash
    roscore
    rosrun urg_node urg_node
\end{lstlisting}

This is a relatively simple example.  As more capability is added to the system, multiple nodes will need to be initiated on launch.  For example - one node to publish laser scanner data, another to publish ultrasonic data, nodes for navigation and visualization.  Having to manually type \texttt{rosrun <package name> <node>} will undoubtedly slow down development time and is prone to errors. \\

ROS provides a solution to this in the form of launch files. Launch files are XML files that can be configured to run multiple nodes distributed across machines.  The benefit of using launch files is that configuring a node to run is a one line XML tag which only needs to be entered once.  In addition to running nodes, topics can be remapped to a different name for one node.  Remapping topics is useful if different manufactures/developers use different topic names to communicate the same data.  Although we haven’t discussed the parameter server in depth, these values can also be set from a launch file. Re-spawning nodes that have died and choosing which machine to run can be achieved easily be specifying simple XML arguments.\\

In our case we need to write a launch file that launches the urg\_node on the Raspberry Pi and the \texttt{rostopic echo} command on the desktop.  Launch files allow us to achieve this from the ground-station.  Figure \ref{fig:launch_file} shows the launch file written to achieve this: 

\begin{figure}[H]
    \centering
    \begin{lstlisting}[language=xml]
    <launch>
    <!-- Setup urg_node -->
    <machine name="drone" address="10.42.0.101" user="pi"/>
    <node machine="drone" name="urg_node" pkg="urg_node" 
        type="urg_node" respawn="true"/>

    <!-- Echo scan data -->
    <node name="echo_scans" pkg="rostopic" type="rostopic" 
        args="echo \scan" respawn="true" output="screen"/>

    <!-- Load robot description file to paramter server -->
    <param name="robot_description" 
        textfile="$(find debug)/urdf/ursa.urdf"/>

    <!-- Use robot description to publish /tf node -->
    <node name="robot_state_publisher" pkg="robot_state_publisher" 
          type="robot_state_publisher"/>
    </launch>
    \end{lstlisting}
    \caption{Launch file to receive laser scanner produced by drone on ground-station}
    \label{fig:launch_file}
\end{figure}

In summary, ROS can be thought of a computation graph made up of nodes.  Nodes communicate with other nodes by publishing messages to topics and receive data by subscribing to topics.  To send laser scan data to the desktop we installed the urg\_node on the Raspberry Pi.   To print laser scan data to the screen we created an echo node on the desktop.  Figure \ref{fig:ROS} captures these high level concepts graphically.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/node_graph.png}
    \caption{ROS nodes, topics and messages}
    \label{fig:ROS}
\end{figure}

\subsection{Visualize laser scanner data}

As shown in Figures \ref{fig:message} and \ref{fig:laser_snippet}, laser scanner data is primarily composed of range measurements updated at \SI{10}{\hertz}. In its raw form it is difficult to interpret the data and confirm that readings received are in fact giving measurements representative of the surroundings.  The next step to be taken is to visualize the range data.\\

ROS has its own visualization tool which can natively subscribe to topics and plot a particular data type.  RVIZ\footnote{http://wiki.ros.org/rviz} recognizes important message types such as laser scans, point clouds, geometric primitives and poses out of the box. The visualization of data is decoupled from the generation of data.  In addition, plugins can be written to display more types of data or provide interface options for users.\\

Laser scans adhering to the sensor\_msgs/Laser\_scan type are supported by RVIZ.  By running our previous launch script and executing the command \texttt{rosrun rviz rviz} we can visualise the point cloud.  Figure \ref{fig:RVIZ_laser} shows the output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/laser_scan_rviz.png}
    \caption{Visualisation of laser scan data}
    \label{fig:RVIZ_laser}
\end{figure}

Additional nodes, such as Cartographer for SLAM, can be setup and configured in a similar way to the laser scanner and RVIZ nodes as discussed above. Note that the full list of ROS nodes launched as part of URSA can be viewed at \url{https://github.com/ursa-drone/ursa-server/blob/master/src/ursa/launch/ursa.launch}.


\section{Project milestone - Hovering in place}
Having developed sensor interfaces, as well as having tested our SLAM solution in simulation and on real data from a handheld source, we were close to the first milestone of URSA. This consists of having the UAV takeoff, hold position in a room using LiDAR scan data for an arbitrary amount of time, and then land. \\

In order to achieve this first project milestone, the following system components needed to be developed and improved:
\begin{itemize}
	\item Providing power to the prototype system
	\item Fusion of the position estimate provided by Cartographer into the EKF used by PX4
	\item Development of a basic user interface for instigation of take-off and landing. In the future, this interface will also provide more advanced navigation and configuration options.
\end{itemize}

Each of these components is discussed below.

\subsection{Power}
Prior to prototype testing, we were able to provide power to the Raspberry Pi via a USB cable from a wall outlet. In an actual flight, the motors also require power, and the ESCs are designed to operate off around \SI{12}{\volt}. The Erle-Copter also includes a buck converter which provides \SI{5}{\volt} power to the Pi from the same \SI{12}{\volt} rail. Batteries were not a viable option for testing due to short flight times and long recharges required.\\

Our first solution was to use a small \SI{12}{\volt} AC to DC regulator. On attempting to take-off using this supply, the motors could not generate enough thrust to lift URSA off the ground. On inspection of the power supply, we found a maximum current rating of \SI{10}{\ampere}. The ESCs on URSA are rated to \SI{30}{\ampere} each, for a total possible current consumption of \SI{120}{\ampere}. Clearly, the first power supply tested was insufficient. \\

We then moved onto a desktop power supply, rated at \SI{600}{\watt}, allowing \SI{50}{\ampere} current output on its \SI{12}{\volt} rail. We found that this power supply provided sufficient current to allow take-off when the LiDAR scanner was not attached, but not with the LiDAR scanner attached.\\

We identified that it was not possible that URSA would require \SI{50}{\ampere} to simply take off and hover, even with around \SI{100}{\gram} load. This can be checked by noting that the provided \SI{5000}{\milli\ampere\hour} battery provides an apparent flight time of 20 minutes. This means expected current consumption should be around \SI{15}{\ampere} for hovering on average. It's unlikely that adding a small weight would increase this to \SI{50}{\ampere}.\\

We measured the voltage at the regulator and compared it to the voltage seen onboard the UAV. We found that the onboard voltage was \SI{12}{\volt} when the motors weren't running; however it sagged to around \SI{8}{\volt} on the UAV when the motors started running. The voltage at the regulator was still \SI{12}{\volt}, even with the motors running. This indicated that the regulator is under no stress when providing the output current, and the problem is in the voltage drop over our cable to the drone. \\

The cable we were using was approximately \SI{4}{\meter} long, advertised for high current, low voltage applications, manufactured by OLEX. Inspecting the conductor dimensions, we found that it was \SI{2.5}{\milli\metre\squared}. This equates to around 13AWG. While we couldn't find the exact cable on the suppliers website, a similar cable\footnote{\url{http://www.olex.com.au/eservice/Australia-en_AU/navigateproduct_540307840/CACP07A1002WVAA.html\#characteristics}} was advertised as having a maximum DC resistance of \SI{7.41}{\ohm\per\kilo\metre}. This gives us a total resistance of \SI{0.059}{\ohm}, including the return path. The voltage drop is therefore expected to be up to \SI{3}{\volt} for the maximum DC output of our power supply.\\

It was clear that the cabling to power our motors was not sufficient. The problem was resolved by replacing the cable with a shorter 8AWG equivalent. This new cable has a resistance approximately $\frac{1}{3}$ the value of a 13AWG cable per unit distance. Using this cable, the voltage drop was reduced to around \SI{1}{volt}, and the prototype was able to lift off with the LiDAR load.

\subsection{EKF Fusion}
The next challenge in achieving stable flight was to feed Cartographer localization information back through the PX4 estimation algorithm. It was straightforward to provide this data over MAVROS using techniques already discussed. Cartographer generates a 2D transform from the origin to the UAV. We wrote a script, \texttt{height\_publisher.py}, which consumes this transform and publishes a new 3D transform which accounts for the data received in the SONAR height sensor, after adjusting for the attitude of the drone. This was then sent to PX4 via the \texttt{vision\_pose\_estimate} topic within MAVROS. \\

The \texttt{vision\_pose\_estimate} topic is designed to send a position and orientation which has been computed by an offboard system. Fusing this data with existing sensors onboard the UAV was slightly more involved and required modifications to the state estimation algorithms within PX4. \\

The default algorithm for state estimation used by PX4 is known as the `local position estimator' (LPE) in conjunction with an attitude estimator. However, this design is in the process of being replaced by a superior EKF which jointly estimates both position and attitude. There are 24 states in the EKF model, and their relationship for the predict/correct EKF equations has been derived symbolically via MATLAB script \footnote{See \url{https://github.com/PX4/ecl/blob/master/matlab/scripts/Inertial\%20Nav\%20EKF/GenerateNavFilterEquations.m}}.\\

The advantage of the EKF over the simpler LPE algorithm is that it supports a so called `delayed fusion time horizon'. This allows the fusion of multiple observation sources which may only be available after a delayed time. This is very useful for GPS data, but can also be re-used for our purposes with Cartographer data. The advantage of either algorithm over a simple averaging filter or using Cartographer data directly is it allows much faster responses to changes in position or attitude, as detected by the IMU onboard URSA. \\

However, the drawback to the EKF used in PX4 is that it is in an experimental state. In particular, the aspect of the system which we propose to use (vision position fusion) is not very well tested. For example, we found that there were overrides when using vision position fusion but not GPS data. This was preventing PX4 generating estimated pose outputs. This is obviously a poor design, as it does not account for cases where `vision estimates' are used in place of GPS.\\

To resolve these issues, we first compiled PX4 to use the experimental EKF instead of the default LPE. We then attempted to configure this environment to work in simulation. We found that some code changes to the EKF were required, and created our own fork of this library. In debugging the code, we found that there were a number of bugs which weren't specific to URSA, but the EKF library generally. We made our improvements available back to the PX4 team\footnote{For example see \url{https://github.com/PX4/ecl/issues/296}}. \\

Eventually, we were able to get the vision position estimation working as required within the EKF. This allowed us to conduct flight tests in simulation.

\subsection{Interface for taking off}
The final aspect of achieving the first milestone was being able to instruct URSA to take-off to a certain height, and to land. We developed a user interface plugin for RVIZ to allow GUI manipulation of URSA. We also built a python script, \texttt{controller.py}. This script was responsible for managing all input and instructions to URSA, including navigation events.\\

Testing showed that the drone was able to take off and hold position. Even when pulled away from the set-point, the drone would return to the same spot. Videos of the tests undertaken for this first milestone are available at \url{https://youtu.be/w-l4K_zTDDQ} and \url{
https://youtu.be/xKCTSxtxNYs}.\\

\section{Navigation}
Our main goal for navigation was to achieve the following:

\begin{itemize}
    \item Mapping stationary obstacles
    \item Avoid both stationary and non-stationary obstacles
    \item Be able to navigate to a set point uploaded by operator
\end{itemize}

We restrict our navigational capabilities to 2D as we are using a planar laser for our primary position estimation tool. We considered 3D scans as out of scope and an area for expansion. \\

A navigation stack which is designed to be customized for different robot types is available in ROS. The navigation stack can be thought of as a black box accepting sensory information (laser scans, odometry readings, height) and a target pose. As output the package sends velocity commands to a mobile base.  The navigation stack is composed of several main components.  We introduce the high-level concepts here before discussing them in more depth.  \\

At the heart of the navigation stack are trajectory planners and costmaps.  Trajectory planners take the current robot position and some goal and generate potential paths. Costmaps influence the selection of optimal path by ensuring that trajectories do not pass through or too close to obstacles.  Both the trajectory planners and costmaps have local and global versions.\\

The global planner takes inputs from the global costmap, goal setpoint and generates the best long term plan.  The local planner considers the local costmap, current laser scan information and position to find the best path along the global plan.  A primary goal of the local plan is to ensure that it avoids obstacles whilst advancing towards the local goal.  In some instances the robot may get stuck.  The navigation stack handles these occurrences by initiating recovery behaviours.\\

Each of these components can be developed separately as long as they adhere to the interface specified in the navcore package.  The move base package connects them together to achieve an overall navigation objective.\\

Figure \ref{fig:nav_stack} shows our setup and summarises the high-level concepts of the navigation stack.  The blue and green coloured boxes highlight the components, which were modified or created by URSA.  In the following subsections we cover the components of the navigation stack in more detail.  We dedicate more focus to those that were modified/added by URSA.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imgs/navigation_stack.png}
    \caption{Core components of the navigation stack\label{fig:nav_stack}}
\end{figure}

\subsection{Global Planner \& Costmap}

The global planner and global costmap required configuration only without any redevelopment of underlying logic. There are three main global planners offered by ROS; carrot, global\_planner and navfn.\\

The carrot planner is the simplest of the three and works by drawing a straight line between robot and target.  If the goal point is an obstacle the carrot planner moves the point closer to the robot until it is no longer over an obstacle.  This type of planner has obvious limitations.  For example a desired capability of the global planner is to be able to place a goal in an unexplored environment.  If an obstacle such as a corner obstructs a desired setpoint the planner wont be able to resolve a valid path.  This is not optimal behaviour as a corner should be navigable.  Even in explored space such a planner would be cumbersome to use as valid paths would have to be identified by the human operator and constructed using straight line segments.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{imgs/carrot_planner_a.png}
        \caption{Navigating a corner}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{imgs/carrot_planner_b.png}
        \caption{Path to be drawn by operator}
    \end{subfigure}
    \caption{Carrot planner\label{fig:carrot}}
\end{figure}

The navfn package works by implementing Dijkstra’s algorithm on the costmap cells to find the optimal path \footnote{See http://wiki.ros.org/navfn}.  Dijkstra’s algorithm is a common “shortest-path algorithm” for weighted graph problems.   Unlike the carrot planner, paths avoiding obstacles can be generated because of the formulation of the problem as a graph.\\

We opted to use the default global planner (global\_planner) that comes with the move base package.  The main difference between the global\_planner implementation and navfn is that an optimised A* algorithm is available in the global\_planner \footnote{See http://wiki.ros.org/global\_planner}.  The A* algorithm is more efficient by employing the use of heuristics.  However, Dijkstra’s algorithm will find the optimal path because it employs breadth first search (considers all points).\\

The local planner is intended to make the final decision on navigation based on the long-term objective expressed by the global planner, the objects in the immediate vicinity of the robot, and the dynamics of the robot. While the global costmap is constructed from a static map generated by Cartographer, the local costmap is created from real time sensor data and therefore better represents dynamic objects.\\

\subsection{Local Planner \& Costmap}

A local planner consists of two main components – trajectory generation and trajectory evaluation.  The trajectory generation takes the global plan as input and produces a number of potential paths.  To select the most eligible option, the paths are compared on a series of cost functions.   Cost functions encode some desired behaviour of a local path such as the avoidance of obstacles or the preference to stay close to the global plan.  The basic operation of the local planner is as follows:

\begin{itemize}
    \item Generate trajectories along global plan
    \item Evaluate trajectories using cost functions
    \item Pick most eligible trajectory
    \item Send velocity commands to mobile base
    \item Repeat
\end{itemize}

The navigation stack is actually intended to work with differential drive or holonomic robots.  Velocity commands (dx, dy, dtheta) are interpretable by these robots and forward simulation is incorporated in local planners to generate trajectories by forward simulation. For URSA, our commands to the controller are positional setpoints. Additionally, the forward simulation for a ground based robot is not an appropriate model for a quadcopter. Therefore the existing local planner is not appropriate. An URSA local planner needs to be created that supports positional setpoints as the output navigational commands to the drone.\\

To create our own URSA local planner we modify the trajectory sample generator and trajectory cost function.\\

\subsubsection{Trajectory Generation} \label{sec: traj_gen}

To create the trajectory generator for our local planner we adhere to the TrajectorySampleGenerator interface specified in the base\_local\_planner namespace \footnote{See http://docs.ros.org/kinetic/api/base\_local\_planner/html/classbase\_\_local\_\_planner\_1\_1TrajectorySampleGenerator.html}.   The interface is relatively simple requiring only two functions (aside from constructors/deconstructors).

\begin{lstlisting}[language=c++]
    virtual bool    hasMoreTrajectories()=0;
    virtual bool    nextTrajectory (Trajectory &traj)=0;
\end{lstlisting}

The purpose of each function is fairly self-explanatory.  The function hasMoreTrajectories returns a Boolean indicating whether the generator is able to create more trajectories. The nextTrajectory function returns the next trajectory via the pointer \&traj.\\

Trajectories are defined as an ordered list of positional coordinates (x, y ,th).  The most logical approach to generate these trajectories is to select some suitable 'endpoints' for our local trajectories and generate a straight line from the drone to these endpoints.  The obvious choice of local trajectory endpoints is to sample the global path at regular intervals.  This method satisfies our goal to advance the drone along the global path.  The entire trajectory is made up by placing regularly spaced points between the current drone position and these endpoints.  We derive the goal heading (th) by using the heading of this straight line.  Figure \ref{fig:local_traj_gen} illustrates these concepts.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/sample_global.png}
        \caption{Sampling global path to generate local trajectories}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/one_local_traj_large.png}
        \caption{Visualization of one local trajectory}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/local_traj_gen.png}
        \caption{Local trajectory generator}
    \end{subfigure}
    \caption{Local trajectory generation\label{fig:local_traj_gen}}
\end{figure}

This approach performed well in most situations however, there were issues under certain circumstances. \\

When drone was nearby the global target the local goal heading could be up to 180 degrees out of phase with the global goal heading \footnote{See https://github.com/ursa-drone/ursa-server/issues/25}.  This is because headings were generated by ‘drawing’ a line between current drone position and local target.  If the drone drifted in front of the global target, the local target would be updated to point in the opposite direction.  This is a critical issue as it would prevent the drone from ever achieving goal position.  To solve this problem only the global path endpoint was used to generate a local trajectory when within some multiplier of the robot footprint.  Therefore the only possible local trajectory was a path to the global plan goal with the same heading.  This resolved the issue.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{imgs/out_of_phase_an.png}
    \caption{Local plan goal out of phase with global plan goal.  (1) Local plan goal (2) Global plan goal (3) Drone footprint\label{fig:lp_gp_out_of_phase}}
\end{figure}

Another issue was identified when observing the drone turn corners in simulation.  Sampling only along the global path results in a restricted set of potential local paths.  This in turn led to unsafe close rounding of corners because the global planner always selects the shortest path as discussed above.  There were other factors involved in close rounding of corners, which are covered in the cost functions section below.  To increase the sample space of potential trajectories, points perpendicular to the global plan were added.  These points were set to be some multiplier away from the global plan.  This solution presented the same issue; therefore the length of perpendicular points was set to be a uniform random variable.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/local_traj_gen_perp.png}
        \caption{Local trajectories with additional perpendicular points}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/local_traj_gen_random.png}
        \caption{Perpendicular points with length as uniform random variable}
    \end{subfigure}
    \caption{Perpendicular points\label{fig:local_traj_gen_perp}}
\end{figure}

Algorithm \ref{alg:trajgen} summarises the operation of the local trajectory generator in pseudocode.\\
\begin{algorithm}[H]
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{vector position, vector global\_traj, vector previous\_traj}
\KwOutput{vector samples, vector previous\_traj}
\KwResult{Given current drone position and global plan, output is a vector of 
positional co-ordinates (x,y,th) for use in generating trajectories}\vspace{1.5mm}

\uIf{global\_traj\_endpoint inside radius}{
samples.push(global\_traj.back());
}
\uElse{
\tcc{Add points along the global plan at 10cm intevals}
\For{i in global\_traj}{
    dist = euclidean\_dist(samples.back(), global\_traj[i])\\
    \If{dist $\geq$ 0.1}{
        samples.push(global\_traj[i])
    }
}\vspace{3.5mm}
\tcc{Generate perpendicular points (random length)}
\For{j in samples}{
    samples.push(perp\_points(samples[j]))
}
}\vspace{3.5mm}
\tcc{Add previous trajectory}
\If{previous\_traj}{
samples.push(previous\_traj.back())
}
\caption{Local trajectory sampling of global plan}
\label{alg:trajgen}
\end{algorithm}

\subsubsection{Cost Functions}
In this section we cover the cost functions utilised by the URSA local planner. These cost functions allow selecting the best trajectory generated by the previous algorithm. Ideally, URSA should follow a trajectory that:
\begin{itemize}
    \item Avoids collisions
    \item Give sufficient clearance to obstacles
    \item Advances drone furthest along global path
    \item Ensures drone is facing in direction of movement
\end{itemize}

All cost functions must adhere to the TrajectoryCostFunction interface. The TrajectoryCostFunction specifies a score function in the format:

\begin{lstlisting}[language=c++]
    virtual double  scoreTrajectory (Trajectory &traj)=0;
\end{lstlisting}

For a given trajectory the cost function returns a score. Negative costs denote a trajectory that has experienced a `lethal' condition (for example, if it has passed through an obstacle) and low scores indicate a preferable trajectory.  For each trajectory the local planner assigns a score by summing the contributions from each cost function.   In this way the cost functions are modular, with the option of adding or removing costs to suit the implementation.  

\paragraph{Obstacle Cost Function}  
The purpose of the obstacle cost function is to prevent the drone from colliding with obstacles. A core concept to understanding how the obstacle cost function is understanding the local costmap model.  The local costmap is generated in realtime from the laser scan data.  From this it creates two layers; the obstacle map layer and the inflation layer. \\

The obstacle map layer places laser scan data in a two dimensional grid.  Cells containing sensor readings are marked as occupied whereas the space between the drone and this reading is marked as clear.\\

The inflation layer creates a buffer around the occupied cells by propagating costs via the procedure outlined below.  Sensor readings indicate obstacles, therefore if the centre point of a robot was inside these cells there would definitely be a collision. These cells are assigned the maximum cost 254 and are referred to as lethal costs.  The next two costs are referred to as inscribed and circumscribed costs.  These names refer to how the costmap handles robots of arbitrary shapes.  The circumscribed region of the robot is a circle passing through the outermost points of the robots perimeter.  The inscribed region is a circle passing through the innermost regions of its perimeter (\ref{fig:circumscribed}).  Obstacles within the inscribed region are definitely in collision with the robot, therefore these cells are assigned a score of 253.  In our case the inscribed and circumscribed regions are the same as we approximate our drone shape by a circle.  Cells at a distance further than the circumscribed radius have their costs determined by a decaying exponential function (Figure \ref{fig:inflation_radius}).

\begin{align*}
    cost\_decay &= 252*\exp(-1 * csf * (dist\_obst - radius))\\
    csf &=    \text{cost scaling factor}\\
    dist\_obst &=    \text{distance from drone centre cell to obstacle}
\end{align*}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{./imgs/circumscribed.jpg}
        \caption{Circumscribed and inscribed radius}
        \label{fig:circumscribed}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{./imgs/inflation_radius.jpg}
        \caption{Illustration of the inflation radius}
        \label{fig:inflation_radius}
    \end{subfigure}
    \caption{Inflation layer\label{fig:inflation_layer}}
\end{figure}

The exponential function can be tuned using the cost scaling factor and inflation radius parameter.  The cost scaling factor controls the rate at which the costs approach 1.  The inflation radius sets a hard boundary for which the inflation layer extends.  Figure \ref{fig:costmap_config} shows various configurations of the inflation layer.  Purple indicates the presence of an obstacle.  If the centre of the robot is in the teal area, the robot radius will have collided with the obstacle.  The gradient red to blue encodes the decaying exponential function with red being 252 and the darkest blue being 1.\\

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/lcm/lcm_csf_1_ir_08.png}
        \caption{csf = 1, ir = 0.8}
    \end{subfigure}%
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/lcm/lcm_csf_553_ir_08.png}
        \caption{csf = 5.53, ir = 0.8}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/lcm/lcm_csf_10_ir_08.png}
        \caption{csf = 10, ir = 0.8}
    \end{subfigure}
        \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/lcm/lcm_csf_1_ir_15.png}
        \caption{csf = 1, ir = 1.5}
    \end{subfigure}%
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/lcm/lcm_csf_553_ir_15.png}
        \caption{csf = 5.53, ir = 1.5}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/lcm/lcm_csf_10_ir_15.png}
        \caption{csf = 10, ir = 1.5}
    \end{subfigure}
    \caption{Various configurations of the cost scaling factor (csf) and inflation radius (ir)}\label{fig:costmap_config}
\end{figure}

The ideal configuration of the costmap could be debated depending on situation and desired properties.  In some instances you may want the drone to take the safest path by preferring to choose paths through the middle of a room.  This could be achieved by setting the inflation radius to a large value so that the whole area is assigned some cost value. \\

In the case that speed to goal is valued it may be preferable to have a smaller inflation radius to result in tighter rounding of corners.  In our implementation we set the inflation radius to be three times the radius.  The cost scaling factor was chosen to give a cost of 1 at $\approx 5.4\cdot radius$
\begin{align*}
    cost\_decay &= 252*\exp(-1 \cdot csf \cdot (dist\_obst - radius))\\
    log(\frac{1}{252}) &= -1 \cdot csf \cdot (5.4\cdot radius - radius)\\
    log(252) &= csf \cdot 4.4\cdot0.5\\
    csf &=    \frac{log(252)}{2.2}
        \approx 2.5
\end{align*}


Having discussed the workings of costmaps we can now cover how they are utilised by the obstacle cost function to produce a score for local trajectories. The obstacle cost function takes a local trajectory, robot footprint and costmap as inputs and returns a score.  The score is calculated by superimposing the footprint on each point of the local trajectory.  Each footprint passes through a set of cells with some associated cost.  The max of all these sets is the score returned (Figure \ref{fig:ocf_default}). \\

This operation of the obstacle cost function lead to the undesired behaviour of rounding corners too close \footnote{See https://github.com/ursa-drone/ursa-server/issues/14}.  One way this behaviour arose was from all local trajectories having the same origin (robot). If this origin was a very high cost, all local trajectories would receive the same max cost so could not be differentiated.  To fix this issue the obstacle cost function was calculated from the beginning of the drone footprint outwards.  Another issue arose from calculating the  obstacle cost by superimposing robot footprints along the local trajectory.  This resulted in the inflation radius being considered twice (inflation radius + robot radius).  Instead the behaviour was changed to return only the max cost at the robot origin \ref{fig:ocf_mod}, since the radius of the robot is already considered in constructing the cost map.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/ocf_default.png}
        \caption{Before modification}
        \label{fig:ocf_default}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/ocf_mod.png}
        \caption{Modified}
        \label{fig:ocf_mod}
    \end{subfigure}
    \caption{Behaviour of obstacle cost function before and after modification}\label{fig:ocf_comp}
\end{figure}

\paragraph{Ursa Goal Cost Function}
Another important specification is for the chosen local trajectory to advance the drone furthest along the global plan.\\

To achieve this the global plan index used to generate a local trajectory was passed through a decaying exponential function.  In this way local trajectories generated from the furthest points along the global plan were prescribed the lowest cost.  \\

% To achieve this Euclidean distance was calculated between the local trajectory endpoint and each point on the global path.  The smallest distance was used to identify the corresponding index along the global path.  A value between 1 and 0 was constructed from this index.  A value of 1 indicated local paths that made no progress along the global path.  A value of 0 indicated that the local plan endpoint was closest to the global plan endpoint.  These values adhere to the assumed operation of cost function prescribing a low score to the most desirable paths.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.4\textwidth]{example-image-a}
%     \caption{Operation of Ursa goal cost function}\label{fig:ugcf}
% \end{figure}

As mentioned above a final score for a local trajectory is determined by summing across all cost functions.  Therefore, the relative scaling between the obstacle cost function and ursa goal cost encodes some implicit preference between distance from obstacle and length of path.  The most desired behaviour being a trajectory that advances furthest along the global plan whilst being far from obstacles.  The least desired behaviour being a trajectory that does not advance very far along the global plan and is set close to obstacles.\\

The difficulty is in choosing the trade-off between path length and distance from obstacles.  Multiple attempts were taken to design alternative cost functions to better control desired behaviour \footnote{See local\_costmap\_slider.m in appendix}.  However the most robust approach was to select the appropriate parameters of the goal cost function using a simplified model (figure \ref{fig:goal_cost_model}). 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imgs/obstacle_vs_goal.png}
    \caption{Model}\label{fig:goal_cost_model}
\end{figure}

Figure \ref{fig:goal_cost_model} shows an obstacle modeled as a point with a surrounding cost determined by the inflation radius.  A global plan was drawn around the obstacle with uniform obstacle cost (zero in the case of a single obstacle).  From the drone's current position a number of straight line local trajectories could be drawn.  The cost of these trajectories is the sum of the obstacle and goal costs.

\begin{align*}
    \text{Obstacle cost function} &=  Ae^{-c\cdot csf}\\
    \text{Goal cost function} &=    Be^{-d\gamma}\\
    \text{Total cost} &=  Ae^{-c\cdot csf} + Be^{-d\gamma}\\
    \end{align*}
A number of these variables have values which are specified by other aspects of the system, such as the cost map implementation:
\begin{align*}
    A &=    252 \tag{\text{From inflation radius definition}}\\
    \alpha &=    \text{robot radius} = 0.5\\
    r &=    \alpha + inflation\_radius = 0.5 + 1.5 = 2\tag{\text{Inflation radius set to 3$\alpha$}}\\
    csf &= 2.5\\
    \intertext{From the model we can find d in terms of c}
    c   &=    rcos(\theta) - \alpha\\
    \theta &=   cos^{-1}(\frac{c+\alpha}{r})\\
    d   &=    2\theta r\\
        &=    2rcos^{-1}(\frac{c+\alpha}{r})\\
\end{align*}
Therefore the only free parameters are c, B and $\gamma$.  As c encodes the distance between the inflation radius and the closest point along the local trajectory, we need to pick a suitable value for $c>0$ that encodes our qualitative preference.  Figure \ref{fig:ocf_total_cost} shows that the minimum total cost can be determined by varying the parameters B and $\lambda$.  Selecting the parameters, B=100 and $\lambda=1$ encodes our preference for the drone to prefer trajectories $\approx 1m$ from an obstacle.  Trajectories close to obstacles are penalised, likewise trajectories too far from an obstacle imply a short trajectory and are therefore also penalised.  It should be noted that $d$ is a monotonically decreasing function of $c$, therefore larger distance from obstacles implies a shorter trajectory.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/obstacle_vs_goal_B_100.jpg}
        \caption{B = 100}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/obstacle_vs_goal_B_504.jpg}
        \caption{B = 504}
    \end{subfigure}
    \caption{Total cost = $Ae^{-c\cdot csf} + Be^{-d\gamma}$ as a funciton of $c, \lambda$ for different B values}\label{fig:ocf_total_cost}
\end{figure}


\paragraph{Turn Before Moving}
Another desired behaviour for our drone is that it is facing the direction it moves.  There are two main reasons for this.  Firstly the laser scanner used only has 240$^\circ$ field of view.  If travelling in the direction of its blind spot there would be nothing preventing collision with obstacles.  Another important factor is the usage of the onboard camera.  Although not used in any position estimation algorithms the camera gives the operator the ability to remotely retrieve images from the environment.\\

This behaviour was implemented using heuristics to modify setpoints which are not in the direction the drone is currently facing.


\end{document}